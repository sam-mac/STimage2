{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = '/scratch/smp/uqsmac12/.conda/env/lit_torch_gp/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2e5d54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/smp/uqsmac12/.conda/env/stimage/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf194bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Q2051'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9d1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from anndata import read_h5ad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fe122f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8f6aa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/smp/uqsmac12/.conda/env/lit_torch_gp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "# from torchvision.transforms import ToTensor\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74889474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the location where models will be saved to\n",
    "if torch.hub.get_dir() == '/clusterdata/uqsmac12/.cache/torch/hub':\n",
    "    torch.hub.set_dir('/scratch/smp/uqsmac12/.cache/torch/hub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec73fba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6432446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup new conda environment\n",
    "\n",
    "# load data\n",
    "\n",
    "# implement somthing similar to STimage1\n",
    "\n",
    "# test likelihood assumption with log [CPT] data transform\n",
    "# test likelihood assumption without transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51505b85",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4fc1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff2f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = '/scratch/smp/uqsmac12/dropin_data'\n",
    "DIR_CHECKPOINTS = '/scratch/smp/uqsmac12/dropin_data/checkpoints'\n",
    "DIR_WANDB = '/scratch/smp/uqsmac12/dropin_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11779f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TILES = '/scratch/smp/uqsmac12/dataset_breast_cancer_9visium'\n",
    "DIR_ANNDATA_PROCESSED = '/scratch/smp/uqsmac12/dataset_breast_cancer_9visium'\n",
    "file_processed_alex_data = 'all_adata.h5ad'\n",
    "# DIR_PROCESSED_DATA = '/afm03/Q2/Q2051/STimage_project/STimage_dataset/PROCESSED/dataset_breast_cancer_9visium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d3ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_all = read_h5ad(os.path.join(DIR_ANNDATA_PROCESSED, file_processed_alex_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "179deb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update metadata for annadata such that it maps to the correct location\n",
    "adata_all.obs[\"tile_path\"] = adata_all.obs.tile_path.map(\n",
    "    lambda x: x.replace(\"/clusterdata/uqxtan9/Xiao/breast_cancer_9visium\",\n",
    "                        DIR_TILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f37bc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list = [\"COX6C\",\"TTLL12\", \"PABPC1\", \"GNAS\", \"HSP90AB1\", \"TFF3\", \"ATP1A1\", \"B2M\", \"FASN\", \"SPARC\", \"CD74\", \"CD63\", \"CD24\", \"CD81\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d00c19",
   "metadata": {},
   "source": [
    "confirm normalisation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c76b5a41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.      , 12.      , 15.000002, 17.      , 12.      , 10.000001,\n",
       "       15.000002,  7.      ,  8.      ,  6.      ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(adata_all[0:10,'COX6C'].to_df().values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dcdb269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    data generator for multiple branches gene prediction model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adata, dim=(299, 299), n_channels=3, genes=None, aug=False, tile_path=\"tile_path\"):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.adata = adata\n",
    "        self.n_channels = n_channels\n",
    "        self.genes = genes\n",
    "        self.num_genes = len(genes)\n",
    "        self.aug = aug\n",
    "        self.tile_path = tile_path\n",
    "        self.indexes = np.arange(self.adata.n_obs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of samples'\n",
    "        return int(self.adata.n_obs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Find obs name\n",
    "        obs_temp = self.adata.obs_names[index]\n",
    "\n",
    "        # Generate data\n",
    "        X_img, y = self._load_data(obs_temp)\n",
    "\n",
    "        return torch.Tensor(X_img), torch.Tensor(y)\n",
    "\n",
    "    def _load_data(self, obs):\n",
    "        img_path = self.adata.obs.loc[obs, 'tile_path']\n",
    "        X_img = Image.open(img_path).convert('RGB')\n",
    "        X_img = transforms.Resize(self.dim)(X_img)\n",
    "        X_img = np.array(X_img).astype('uint8')\n",
    "        #         X_img = np.expand_dims(X_img, axis=0)\n",
    "        #         n_rotate = np.random.randint(0, 4)\n",
    "        #         X_img = np.rot90(X_img, k=n_rotate, axes=(1, 2))\n",
    "        if self.aug:\n",
    "            X_img = seq_aug(image=X_img)\n",
    "        y = self._load_label(obs)\n",
    "        return X_img, y\n",
    "\n",
    "    def _load_label(self, obs):\n",
    "        batch_adata = self.adata[obs, self.genes].copy()\n",
    "\n",
    "        return tuple([batch_adata.to_df()[i].values for i in self.genes])\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.adata.to_df().loc[:, self.genes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334758a",
   "metadata": {},
   "source": [
    "# Pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0b3b8f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TORCH_HOME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTORCH_HOME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/scratch/smp/uqsmac12/.conda/env/lit_torch_gp/lib/python3.8/os.py:675\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    672\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TORCH_HOME'"
     ]
    }
   ],
   "source": [
    "os.environ['TORCH_HOME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfcf9cf",
   "metadata": {},
   "source": [
    "# lit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83c043b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ebcab75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/clusterdata/uqsmac12/.cache/torch/hub'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f4ee686",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.set_dir('/scratch/smp/uqsmac12/.cache/torch/hub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15716645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/smp/uqsmac12/.cache/torch/hub'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hub.get_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7218a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = torchvision.models.resnet18(weights=\"DEFAULT\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "125d1b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetTransferLearning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        backbone = torchvision.models.resnet18(weights=\"DEFAULT\")\n",
    "        num_filters = backbone.fc.in_features\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        \n",
    "        # use the pretrained model to classify cifar10\n",
    "        num_target_classes = 10\n",
    "        self.classifier = nn.Linear(num_filters, num_target_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            representations = self.feature_extractor(x).flatten(1)\n",
    "        x = self.classifier(representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ea8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune\n",
    "model = ImagenetTransferLearning()\n",
    "trainer = Trainer()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use to predict data of interest\n",
    "# model = ImagenetTransferLearning.load_from_checkpoint(PATH)\n",
    "# model.freeze()\n",
    "\n",
    "# x = some_images_from_cifar10()\n",
    "# predictions = model(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e49b164e",
   "metadata": {},
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning module is full recipe defining how modules interact\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['encoder', 'decoder']) # 'gives model attribute 'hyper_parameters'\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1) # reshape since autoencoder is mlp\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if trainer.global_step == 0:\n",
    "            wandb.define_metric('val_loss', summary='min')\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", val_loss) # what is monitored\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "\n",
    "    def configure_optimizers(self, lr=1e-3):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536c871",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project=\"Stimage2\", \n",
    "    log_model=\"all\",\n",
    "    save_dir=DIR_WANDB,\n",
    "    name='stimage1_likelihood_sensitivity' # run names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_logger.watch(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400c262",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b362795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset\n",
    "# train_set = MNIST(root=DIR_DATA, download=True, transform=transforms.ToTensor())\n",
    "# test_set = MNIST(root=DIR_DATA, download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# # use 20% of training data for validation\n",
    "# train_set_size = int(len(train_set) * 0.8)\n",
    "# valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# # split the train set into two\n",
    "# seed = torch.Generator().manual_seed(42)\n",
    "# train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "# # dataloaders\n",
    "# train_loader = DataLoader(train_set, batch_size=256, num_workers=8)\n",
    "# valid_loader = DataLoader(valid_set, batch_size=256, num_workers=8)\n",
    "# test_loader = DataLoader(test_set, batch_size=256, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59871af",
   "metadata": {},
   "source": [
    "# trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f22ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3,)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=None, monitor=\"val_loss\", save_top_k=1, every_n_epochs=None\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=DIR_CHECKPOINTS, \n",
    "    enable_checkpointing=True, \n",
    "    accelerator='gpu', \n",
    "    logger=wandb_logger,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc19d4",
   "metadata": {},
   "source": [
    "# SUPERSEDED"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cab29f84",
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "from typing import Union, Dict, Optional, Tuple, BinaryIO\n",
    "import h5py\n",
    "import json\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import anndata\n",
    "from anndata import (\n",
    "    AnnData,\n",
    "    read_csv,\n",
    "    read_text,\n",
    "    read_excel,\n",
    "    read_mtx,\n",
    "    read_loom,\n",
    "    read_hdf,\n",
    ")\n",
    "from anndata import read as read_h5ad\n",
    "from anndata import read_h5ad\n",
    "import scanpy as sc\n",
    "from scanpy import read_visium, read_10x_mtx\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd \n",
    "# import scprep as scp\n",
    "import anndata as ad\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile, Image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lit_torch_gp",
   "language": "python",
   "name": "lit_torch_gp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
